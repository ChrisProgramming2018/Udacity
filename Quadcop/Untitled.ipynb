{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from task import Task\n",
    "\n",
    "class PolicySearch_Agent():\n",
    "    def __init__(self, task):\n",
    "        # Task (environment) information\n",
    "        self.task = task\n",
    "        self.state_size = task.state_size\n",
    "        self.action_size = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        self.w = np.random.normal(\n",
    "            size=(self.state_size, self.action_size),  # weights for simple linear policy: state_space x action_space\n",
    "            scale=(self.action_range / (2 * self.state_size))) # start producing actions in a decent range\n",
    "\n",
    "        # Score tracker and learning parameters\n",
    "        self.best_w = None\n",
    "        self.best_score = -np.inf\n",
    "        self.noise_scale = 0.1\n",
    "\n",
    "        # Episode variables\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.count = 0\n",
    "        state = self.task.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, reward, done):\n",
    "        # Save experience / reward\n",
    "        self.total_reward += reward\n",
    "        self.count += 1\n",
    "\n",
    "        # Learn, if at end of episode\n",
    "        if done:\n",
    "            self.learn()\n",
    "\n",
    "    def act(self, state):\n",
    "        # Choose action based on given state and policy\n",
    "        action = np.dot(state, self.w)  # simple linear policy\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # Learn by random policy search, using a reward-based score\n",
    "        self.score = self.total_reward / float(self.count) if self.count else 0.0\n",
    "        if self.score > self.best_score:\n",
    "            self.best_score = self.score\n",
    "            self.best_w = self.w\n",
    "            self.noise_scale = max(0.5 * self.noise_scale, 0.01)\n",
    "        else:\n",
    "            self.w = self.best_w\n",
    "            self.noise_scale = min(2.0 * self.noise_scale, 3.2)\n",
    "        self.w = self.w + self.noise_scale * np.random.normal(size=self.w.shape)  # equal noise in all directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PolicySearch_Agent(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = p.reset_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.72194772, 260.00572761, 686.7139676 , 652.4764742 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.act(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
