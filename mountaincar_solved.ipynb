{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-13 12:31:07,047] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of possible actions:', 3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55614474,  0.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, \n",
    "                 action_size=3, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "round(state[0],3)\n",
    "round(state[1],3)\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    round(next_state[0], 3)\n",
    "    round(next_state[1], 3)\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def reward_function(state, action):\n",
    "    '''  '''\n",
    "    #0 push left\n",
    "    #1 no push\n",
    "    #2 push right\n",
    "    # going the right state [0] positive and positiv velocity \n",
    "    veloctiy = state[1]\n",
    "    if state[0] > 0 and veloctiy > 0:\n",
    "        if action == 0:\n",
    "            reward = -20\n",
    "        if action == 1:\n",
    "            reward = -20\n",
    "        if action == 2:\n",
    "            reward = 20\n",
    "        return reward\n",
    "    # its on the right going to the left\n",
    "    if state[0] > 0 and veloctiy <= 0:\n",
    "        if action == 0:\n",
    "            reward = 20\n",
    "        if action == 1:\n",
    "            reward = -20\n",
    "        if action == 2:\n",
    "            reward = -20\n",
    "        return reward\n",
    "    # its on the left \n",
    "    if state[0] <= 0 and veloctiy > 0:\n",
    "        if action == 0:\n",
    "            reward = 20\n",
    "        if action == 1:\n",
    "            reward = -20\n",
    "        if action == 2:\n",
    "            reward = -20\n",
    "        return reward\n",
    "        # its on the left \n",
    "    if state[0] <= 0 and veloctiy <= 0:\n",
    "        if action == 0:\n",
    "            reward = -20\n",
    "        if action == 1:\n",
    "            reward = -20\n",
    "        if action == 2:\n",
    "            reward = 20\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(state, action):\n",
    "    '''  '''\n",
    "    #0 push left\n",
    "    #1 no push\n",
    "    #2 push right\n",
    "    # going the right state [0] positive and positiv velocity \n",
    "    veloctiy = state[1]\n",
    "    if veloctiy > 0:\n",
    "        if action == 0:\n",
    "            reward = -1\n",
    "        if action == 1:\n",
    "            reward = -1\n",
    "        if action == 2:\n",
    "            reward = 1\n",
    "        return reward\n",
    "    # its on the right going to the left\n",
    "    if  veloctiy <= 0:\n",
    "        if action == 0:\n",
    "            reward = 1\n",
    "        if action == 1:\n",
    "            reward = -1\n",
    "        if action == 2:\n",
    "            reward = -1\n",
    "    if state[0] > 0.2:\n",
    "        reward += state[0]\n",
    "    return np.tanh(reward)\n",
    "    #return np.tanh(reward * abs(state[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41499999999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = abs(state[0])\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.76159415595576485"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_func(state, action):\n",
    "    '''  '''\n",
    "    print(state)\n",
    "    if state[0] >= 0:\n",
    "        print('right ', end='')\n",
    "    else:\n",
    "        print('left ', end='')\n",
    "    if state[1] >= 0:\n",
    "        print('positiv ', end='')\n",
    "    else:\n",
    "        print('negative ', end='')\n",
    "    if action == 0:\n",
    "        print(' push left')\n",
    "    if action == 1:\n",
    "        print(' no push ')\n",
    "    if action == 2:\n",
    "        print(' push right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end episode -0.279\n",
      "Done  171\n",
      "[-0.32   0.015]\n",
      "Episode: 1 Total reward: -180.516665587 Training loss: 0.8116 Explore P: 0.9831\n",
      "end episode -0.319\n",
      "Episode: 2 Total reward: -26.8014480548 Training loss: 0.7443 Explore P: 0.9639\n",
      "end episode -0.412\n",
      "Episode: 3 Total reward: -42.5101428621 Training loss: 0.8024 Explore P: 0.9451\n",
      "end episode -0.307\n",
      "Episode: 4 Total reward: -19.9007240274 Training loss: 0.6830 Explore P: 0.9267\n",
      "end episode -0.298\n",
      "Episode: 5 Total reward: -45.1855064953 Training loss: 1.8057 Explore P: 0.9087\n",
      "end episode -0.445\n",
      "Episode: 6 Total reward: -45.3710129906 Training loss: 5.6177 Explore P: 0.8910\n",
      "end episode -0.24\n",
      "Episode: 7 Total reward: -41.0398538989 Training loss: 1.0052 Explore P: 0.8736\n",
      "end episode -0.414\n",
      "Episode: 8 Total reward: -31.7152175321 Training loss: 16.4066 Explore P: 0.8566\n",
      "end episode -0.444\n",
      "Episode: 9 Total reward: -29.3311590917 Training loss: 79.2301 Explore P: 0.8399\n",
      "end episode -0.403\n",
      "Episode: 10 Total reward: -58.2318831191 Training loss: 1.9026 Explore P: 0.8235\n",
      "end episode -0.399\n",
      "Episode: 11 Total reward: -23.0927532476 Training loss: 4.0851 Explore P: 0.8075\n",
      "end episode -0.404\n",
      "Episode: 12 Total reward: -26.4304350643 Training loss: 31.4719 Explore P: 0.7918\n",
      "end episode -0.387\n",
      "Episode: 13 Total reward: -43.238405844 Training loss: 3.9773 Explore P: 0.7764\n",
      "end episode -0.397\n",
      "Episode: 14 Total reward: -32.5231883119 Training loss: 4.0082 Explore P: 0.7613\n",
      "end episode -0.374\n",
      "Episode: 15 Total reward: -33.3311590917 Training loss: 6.9001 Explore P: 0.7465\n",
      "end episode -0.47\n",
      "Episode: 16 Total reward: -37.4768116881 Training loss: 4.2882 Explore P: 0.7320\n",
      "end episode -0.342\n",
      "Episode: 17 Total reward: -14.483334413 Training loss: 7.7147 Explore P: 0.7178\n",
      "end episode -0.425\n",
      "Episode: 18 Total reward: -26.8144935047 Training loss: 9.3919 Explore P: 0.7038\n",
      "end episode -0.378\n",
      "Episode: 19 Total reward: -30.0065227249 Training loss: 8.0521 Explore P: 0.6901\n",
      "end episode -0.457\n",
      "Episode: 20 Total reward: -20.8144935047 Training loss: 8.8107 Explore P: 0.6767\n",
      "end episode -0.399\n",
      "Episode: 21 Total reward: -28.4304350643 Training loss: 6.9763 Explore P: 0.6636\n",
      "end episode -0.39\n",
      "Episode: 22 Total reward: -18.6289870094 Training loss: 11.5909 Explore P: 0.6507\n",
      "end episode -0.463\n",
      "Episode: 23 Total reward: -36.0992759726 Training loss: 7.4872 Explore P: 0.6381\n",
      "end episode -0.398\n",
      "Episode: 24 Total reward: -22.8144935047 Training loss: 4.3478 Explore P: 0.6257\n",
      "end episode -0.495\n",
      "Episode: 25 Total reward: -42.0065227249 Training loss: 5.2505 Explore P: 0.6136\n",
      "end episode -0.42\n",
      "Episode: 26 Total reward: -14.244928569 Training loss: 3.6309 Explore P: 0.6017\n",
      "end episode -0.322\n",
      "Episode: 27 Total reward: -30.483334413 Training loss: 3.0369 Explore P: 0.5900\n",
      "end episode -0.434\n",
      "Episode: 28 Total reward: -10.7746396058 Training loss: 4.2565 Explore P: 0.5786\n",
      "end episode -0.414\n",
      "Episode: 29 Total reward: -34.1521753213 Training loss: 3.2185 Explore P: 0.5674\n",
      "end episode -0.357\n",
      "Episode: 30 Total reward: -24.0594220737 Training loss: 4.6279 Explore P: 0.5564\n",
      "end episode -0.406\n",
      "Episode: 31 Total reward: -5.2514512939 Training loss: 5.0033 Explore P: 0.5457\n",
      "end episode -0.396\n",
      "Episode: 32 Total reward: -9.01304544986 Training loss: 4.5479 Explore P: 0.5351\n",
      "end episode -0.405\n",
      "Episode: 33 Total reward: -16.2050746701 Training loss: 3.2923 Explore P: 0.5248\n",
      "end episode -0.489\n",
      "Episode: 34 Total reward: -16.7746396058 Training loss: 3.7378 Explore P: 0.5146\n",
      "end episode -0.453\n",
      "Episode: 35 Total reward: -15.3971038903 Training loss: 3.1408 Explore P: 0.5047\n",
      "end episode -0.504\n",
      "Episode: 36 Total reward: 5.41086688948 Training loss: 255.6711 Explore P: 0.4949\n",
      "end episode -0.382\n",
      "Episode: 37 Total reward: -36.0594220737 Training loss: 3.5520 Explore P: 0.4854\n",
      "end episode -0.501\n",
      "Episode: 38 Total reward: -21.0130454499 Training loss: 1.4708 Explore P: 0.4760\n",
      "end episode -0.452\n",
      "Episode: 39 Total reward: -20.3507272665 Training loss: 1.2086 Explore P: 0.4668\n",
      "end episode -0.423\n",
      "Episode: 40 Total reward: -16.5891331105 Training loss: 2.5109 Explore P: 0.4578\n",
      "end episode -0.119\n",
      "Episode: 41 Total reward: -23.8210162296 Training loss: 1.0808 Explore P: 0.4490\n",
      "end episode -0.404\n",
      "Episode: 42 Total reward: -47.9137694773 Training loss: 1.2519 Explore P: 0.4404\n",
      "end episode -0.21\n",
      "Episode: 43 Total reward: -19.728262982 Training loss: 1.7824 Explore P: 0.4319\n",
      "end episode -0.159\n",
      "Episode: 44 Total reward: -14.5891331105 Training loss: 2.7771 Explore P: 0.4236\n",
      "end episode -0.07\n",
      "Episode: 45 Total reward: -18.7217402571 Training loss: 1.2114 Explore P: 0.4154\n",
      "end episode -0.013\n",
      "Episode: 46 Total reward: -21.8210162296 Training loss: 1.3267 Explore P: 0.4074\n",
      "end episode -0.533\n",
      "Episode: 47 Total reward: -2.49637986288 Training loss: 0.8989 Explore P: 0.3996\n",
      "end episode -0.114\n",
      "Episode: 48 Total reward: -18.7217402571 Training loss: 20.4945 Explore P: 0.3919\n",
      "end episode -0.134\n",
      "Episode: 49 Total reward: -3.06594479861 Training loss: 0.8183 Explore P: 0.3844\n",
      "end episode -0.117\n",
      "Episode: 50 Total reward: -15.1586980463 Training loss: 111.1695 Explore P: 0.3770\n",
      "end episode -0.11\n",
      "Episode: 51 Total reward: -9.34420454155 Training loss: 0.8275 Explore P: 0.3698\n",
      "end episode 0.032\n",
      "Episode: 52 Total reward: 1.98043182521 Training loss: 0.7924 Explore P: 0.3627\n",
      "end episode -0.016\n",
      "Episode: 53 Total reward: -8.44348051412 Training loss: 0.6826 Explore P: 0.3557\n",
      "end episode -0.038\n",
      "Episode: 54 Total reward: 5.74202598117 Training loss: 1.1330 Explore P: 0.3489\n",
      "end episode -0.041\n",
      "Episode: 55 Total reward: 10.0731850729 Training loss: 0.9067 Explore P: 0.3423\n",
      "end episode 0.105\n",
      "Episode: 56 Total reward: 10.9340552014 Training loss: 26.0469 Explore P: 0.3357\n",
      "end episode 0.017\n",
      "Episode: 57 Total reward: 12.788402605 Training loss: 1.0783 Explore P: 0.3293\n",
      "end episode 0.191\n",
      "Episode: 58 Total reward: 23.6492727335 Training loss: 0.8101 Explore P: 0.3230\n",
      "end episode 0.223\n",
      "Episode: 59 Total reward: 36.6350862653 Training loss: 1.3961 Explore P: 0.3168\n",
      "end episode -0.01\n",
      "Episode: 60 Total reward: 41.2652142931 Training loss: 0.9185 Explore P: 0.3108\n",
      "end episode 0.05\n",
      "Episode: 61 Total reward: 17.8876785776 Training loss: 2.4967 Explore P: 0.3049\n",
      "end episode 0.053\n",
      "Episode: 62 Total reward: 25.4108668895 Training loss: 0.8820 Explore P: 0.2991\n",
      "end episode -0.004\n",
      "Episode: 63 Total reward: 26.6028961097 Training loss: 61.3492 Explore P: 0.2934\n",
      "end episode -0.065\n",
      "Episode: 64 Total reward: 28.3644902657 Training loss: 3.8766 Explore P: 0.2878\n",
      "end episode 0.014\n",
      "Episode: 65 Total reward: 16.4572435133 Training loss: 1.1153 Explore P: 0.2823\n",
      "end episode -0.122\n",
      "Episode: 66 Total reward: 1.31811364183 Training loss: 1.5871 Explore P: 0.2769\n",
      "end episode -0.544\n",
      "Episode: 67 Total reward: 7.92753247646 Training loss: 1.6084 Explore P: 0.2717\n",
      "end episode 0.14\n",
      "Episode: 68 Total reward: 21.1724610454 Training loss: 1.1101 Explore P: 0.2665\n",
      "end episode -0.092\n",
      "Episode: 69 Total reward: 22.5499967609 Training loss: 1.0063 Explore P: 0.2615\n",
      "end episode -0.073\n",
      "Episode: 70 Total reward: 23.8876785776 Training loss: 1.2077 Explore P: 0.2565\n",
      "end episode -0.148\n",
      "Episode: 71 Total reward: 18.3115909169 Training loss: 1.3400 Explore P: 0.2517\n",
      "end episode -0.111\n",
      "Episode: 72 Total reward: -1.87391557839 Training loss: 1.2861 Explore P: 0.2469\n",
      "end episode -0.259\n",
      "Episode: 73 Total reward: 25.4507207884 Training loss: 61.4971 Explore P: 0.2422\n",
      "end episode -0.101\n",
      "Episode: 74 Total reward: -3.21159739501 Training loss: 2.2094 Explore P: 0.2376\n",
      "end episode -0.035\n",
      "Episode: 75 Total reward: -10.1123214224 Training loss: 71.2780 Explore P: 0.2332\n",
      "end episode -0.092\n",
      "Episode: 76 Total reward: 5.88767857757 Training loss: 1.3497 Explore P: 0.2288\n",
      "end episode -0.153\n",
      "Episode: 77 Total reward: 20.5499967609 Training loss: 1.4957 Explore P: 0.2245\n",
      "end episode -0.049\n",
      "Episode: 78 Total reward: -26.2050746701 Training loss: 5.6074 Explore P: 0.2202\n",
      "end episode 0.005\n",
      "Episode: 79 Total reward: -5.78116233074 Training loss: 2.6221 Explore P: 0.2161\n",
      "end episode -0.232\n",
      "Episode: 80 Total reward: 18.2188376693 Training loss: 2.0013 Explore P: 0.2120\n",
      "end episode -0.12\n",
      "Episode: 81 Total reward: 15.1724610454 Training loss: 2.7959 Explore P: 0.2080\n",
      "end episode -0.025\n",
      "Episode: 82 Total reward: -3.3971038903 Training loss: 1.7585 Explore P: 0.2041\n",
      "end episode 0.027\n",
      "Episode: 83 Total reward: -18.8673928535 Training loss: 1.3251 Explore P: 0.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end episode -0.094\n",
      "Episode: 84 Total reward: 48.3115909169 Training loss: 1.6921 Explore P: 0.1966\n",
      "end episode -0.249\n",
      "Episode: 85 Total reward: -23.7811623307 Training loss: 3.1690 Explore P: 0.1929\n",
      "end episode 0.082\n",
      "Episode: 86 Total reward: 27.0797077978 Training loss: 1.5885 Explore P: 0.1893\n",
      "end episode 0.068\n",
      "Episode: 87 Total reward: 31.7949253299 Training loss: 3.4172 Explore P: 0.1858\n",
      "end episode 0.23\n",
      "Episode: 88 Total reward: 57.8698854872 Training loss: 1.2465 Explore P: 0.1823\n",
      "end episode 0.154\n",
      "Episode: 89 Total reward: 35.5565194859 Training loss: 2.8845 Explore P: 0.1789\n",
      "end episode -0.028\n",
      "Episode: 90 Total reward: 9.46376623823 Training loss: 2.0999 Explore P: 0.1756\n",
      "end episode 0.045\n",
      "Episode: 91 Total reward: 54.8413019537 Training loss: 1.0539 Explore P: 0.1723\n",
      "end episode 0.214\n",
      "Episode: 92 Total reward: 48.9869545501 Training loss: 1.0893 Explore P: 0.1691\n",
      "end episode -0.24\n",
      "Episode: 93 Total reward: 32.8811558526 Training loss: 1.8581 Explore P: 0.1660\n",
      "end episode 0.215\n",
      "Episode: 94 Total reward: 49.7021720823 Training loss: 1.2147 Explore P: 0.1629\n",
      "end episode 0.262\n",
      "Episode: 95 Total reward: 34.4702889632 Training loss: 0.9166 Explore P: 0.1599\n",
      "end episode -0.107\n",
      "Episode: 96 Total reward: 29.755071431 Training loss: 1.2777 Explore P: 0.1569\n",
      "end episode 0.116\n",
      "Episode: 97 Total reward: 49.1326071465 Training loss: 1.2540 Explore P: 0.1540\n",
      "end episode -0.093\n",
      "Episode: 98 Total reward: 43.9007240274 Training loss: 2.2964 Explore P: 0.1512\n",
      "end episode 0.009\n",
      "Episode: 99 Total reward: 34.8543474036 Training loss: 2.1656 Explore P: 0.1484\n",
      "end episode 0.224\n",
      "Episode: 100 Total reward: 38.5495249504 Training loss: 2.2490 Explore P: 0.1457\n",
      "end episode 0.128\n",
      "Episode: 101 Total reward: 46.2318831191 Training loss: 2.0439 Explore P: 0.1430\n",
      "end episode 0.073\n",
      "Episode: 102 Total reward: 36.6159415596 Training loss: 2.0574 Explore P: 0.1404\n",
      "end episode 0.329\n",
      "Episode: 103 Total reward: 22.0635657104 Training loss: 0.8281 Explore P: 0.1378\n",
      "end episode -0.042\n",
      "Episode: 104 Total reward: 59.0927532476 Training loss: 2.4562 Explore P: 0.1353\n",
      "end episode 0.035\n",
      "Episode: 105 Total reward: 55.8079707798 Training loss: 0.9463 Explore P: 0.1328\n",
      "end episode -0.04\n",
      "Episode: 106 Total reward: 66.6688409083 Training loss: 1.3972 Explore P: 0.1304\n",
      "end episode 0.493\n",
      "Episode: 107 Total reward: 26.56192079 Training loss: 1.5141 Explore P: 0.1280\n",
      "end episode -0.267\n",
      "Episode: 108 Total reward: 30.5492792116 Training loss: 2.6592 Explore P: 0.1257\n",
      "end episode -0.304\n",
      "Episode: 109 Total reward: 21.0260908997 Training loss: 2.6254 Explore P: 0.1234\n",
      "end episode 0.161\n",
      "Episode: 110 Total reward: 65.2514512939 Training loss: 2.1862 Explore P: 0.1212\n",
      "end episode -0.312\n",
      "Episode: 111 Total reward: -5.96738637536 Training loss: 1.5135 Explore P: 0.1190\n",
      "end episode -0.316\n",
      "Episode: 112 Total reward: 19.1717434961 Training loss: 1.7583 Explore P: 0.1169\n",
      "end episode -0.18\n",
      "Episode: 113 Total reward: 37.5956558355 Training loss: 1.1414 Explore P: 0.1148\n",
      "end episode -0.262\n",
      "Episode: 114 Total reward: 31.1188441474 Training loss: 1.3842 Explore P: 0.1127\n",
      "end episode -0.303\n",
      "Episode: 115 Total reward: 23.1717434961 Training loss: 1.8625 Explore P: 0.1107\n",
      "end episode -0.201\n",
      "Episode: 116 Total reward: 31.5029025878 Training loss: 1.9601 Explore P: 0.1087\n",
      "end episode -0.377\n",
      "Episode: 117 Total reward: -38.2456461183 Training loss: 1.2264 Explore P: 0.1067\n",
      "end episode -0.232\n",
      "Episode: 118 Total reward: 45.5956558355 Training loss: 1.0876 Explore P: 0.1048\n",
      "end episode -0.166\n",
      "Episode: 119 Total reward: 31.9797142759 Training loss: 1.6729 Explore P: 0.1030\n",
      "end episode -0.208\n",
      "Episode: 120 Total reward: 58.8275389546 Training loss: 1.1720 Explore P: 0.1011\n",
      "end episode -0.29\n",
      "Episode: 121 Total reward: 32.5492792116 Training loss: 1.4446 Explore P: 0.0993\n",
      "end episode 0.461\n",
      "Episode: 122 Total reward: 66.6289870094 Training loss: 1.2691 Explore P: 0.0976\n",
      "end episode -0.321\n",
      "Episode: 123 Total reward: 40.0724675235 Training loss: 1.2594 Explore P: 0.0959\n",
      "end episode 0.415\n",
      "Episode: 124 Total reward: 62.5362337618 Training loss: 1.4408 Explore P: 0.0942\n",
      "end episode 0.5\n",
      "Done  196\n",
      "[ 0.492  0.008]\n",
      "Episode: 125 Total reward: -138.563042211 Training loss: 171.5922 Explore P: 0.0925\n",
      "end episode -0.239\n",
      "Episode: 126 Total reward: 38.4036266152 Training loss: 1.5289 Explore P: 0.0909\n",
      "end episode 0.09\n",
      "Episode: 127 Total reward: 67.8739155784 Training loss: 1.0699 Explore P: 0.0893\n",
      "end episode -0.219\n",
      "Episode: 128 Total reward: 45.8340616795 Training loss: 1.2341 Explore P: 0.0877\n",
      "end episode -0.273\n",
      "Episode: 129 Total reward: 48.0724675235 Training loss: 1.4059 Explore P: 0.0862\n",
      "end episode -0.327\n",
      "Episode: 130 Total reward: 16.694931808 Training loss: 1.7451 Explore P: 0.0847\n",
      "end episode -0.3\n",
      "Episode: 131 Total reward: 36.3108733676 Training loss: 0.6408 Explore P: 0.0832\n",
      "end episode -0.166\n",
      "Episode: 132 Total reward: 47.1188441474 Training loss: 0.9826 Explore P: 0.0818\n",
      "end episode -0.264\n",
      "Episode: 133 Total reward: 31.0260908997 Training loss: 1.1670 Explore P: 0.0804\n",
      "end episode -0.264\n",
      "Episode: 134 Total reward: 45.9268149271 Training loss: 1.2515 Explore P: 0.0790\n",
      "end episode 0.14\n",
      "Episode: 135 Total reward: 57.728262982 Training loss: 1.2647 Explore P: 0.0776\n",
      "end episode -0.089\n",
      "Episode: 136 Total reward: 47.3572499914 Training loss: 1.0698 Explore P: 0.0763\n",
      "end episode -0.217\n",
      "Episode: 137 Total reward: 38.5492792116 Training loss: 1.3346 Explore P: 0.0750\n",
      "end episode 0.172\n",
      "Episode: 138 Total reward: 25.211597395 Training loss: 1.1148 Explore P: 0.0737\n",
      "end episode 0.51\n",
      "Done  193\n",
      "[ 0.468  0.041]\n",
      "Episode: 139 Total reward: -97.4702889632 Training loss: 1.3773 Explore P: 0.0725\n",
      "end episode -0.292\n",
      "Episode: 140 Total reward: 41.5029025878 Training loss: 1.0354 Explore P: 0.0713\n",
      "end episode -0.301\n",
      "Episode: 141 Total reward: 36.7876850557 Training loss: 1.5366 Explore P: 0.0701\n",
      "end episode 0.513\n",
      "Done  197\n",
      "[ 0.471  0.042]\n",
      "Episode: 142 Total reward: -98.4239123393 Training loss: 1.2269 Explore P: 0.0689\n",
      "end episode 0.519\n",
      "Done  196\n",
      "[ 0.475  0.043]\n",
      "Episode: 143 Total reward: -102.947100651 Training loss: 40.9854 Explore P: 0.0677\n",
      "end episode 0.537\n",
      "Done  187\n",
      "[ 0.49   0.046]\n",
      "Episode: 144 Total reward: -71.2847824679 Training loss: 0.6886 Explore P: 0.0667\n",
      "end episode 0.517\n",
      "Done  195\n",
      "[ 0.493  0.023]\n",
      "Episode: 145 Total reward: -91.4702889632 Training loss: 1.1719 Explore P: 0.0656\n",
      "end episode -0.311\n",
      "Episode: 146 Total reward: 35.5029025878 Training loss: 0.8457 Explore P: 0.0645\n",
      "end episode 0.521\n",
      "Done  186\n",
      "[ 0.486  0.035]\n",
      "Episode: 147 Total reward: -74.5231883119 Training loss: 0.9213 Explore P: 0.0634\n",
      "end episode 0.508\n",
      "Done  189\n",
      "[ 0.461  0.047]\n",
      "Episode: 148 Total reward: -54.3840584404 Training loss: 1.3909 Explore P: 0.0624\n",
      "end episode -0.351\n",
      "Episode: 149 Total reward: 12.3637727163 Training loss: 0.9362 Explore P: 0.0614\n",
      "end episode -0.307\n",
      "Episode: 150 Total reward: 29.7413084318 Training loss: 1.0393 Explore P: 0.0604\n",
      "end episode -0.361\n",
      "Episode: 151 Total reward: -7.49057468727 Training loss: 1.1850 Explore P: 0.0594\n",
      "end episode 0.52\n",
      "Done  187\n",
      "[ 0.486  0.033]\n",
      "Episode: 152 Total reward: -67.761594156 Training loss: 0.9331 Explore P: 0.0585\n",
      "end episode -0.242\n",
      "Episode: 153 Total reward: 39.9797142759 Training loss: 1.2084 Explore P: 0.0575\n",
      "end episode 0.222\n",
      "Episode: 154 Total reward: 79.3358146914 Training loss: 1.0342 Explore P: 0.0566\n",
      "end episode 0.515\n",
      "Done  179\n",
      "[ 0.468  0.048]\n",
      "Episode: 155 Total reward: -53.2289986011 Training loss: 1.0274 Explore P: 0.0558\n",
      "end episode 0.275\n",
      "Episode: 156 Total reward: 84.0594220737 Training loss: 0.9836 Explore P: 0.0549\n",
      "end episode 0.248\n",
      "Episode: 157 Total reward: 74.385437946 Training loss: 1.0787 Explore P: 0.0540\n",
      "end episode -0.318\n",
      "Episode: 158 Total reward: 40.3108733676 Training loss: 0.9940 Explore P: 0.0531\n",
      "end episode 0.535\n",
      "Done  168\n",
      "[ 0.489  0.045]\n",
      "Episode: 159 Total reward: -25.7681168809 Training loss: 0.8547 Explore P: 0.0524\n",
      "end episode -0.325\n",
      "Episode: 160 Total reward: 35.5029025878 Training loss: 0.8158 Explore P: 0.0516\n",
      "end episode 0.526\n",
      "Done  183\n",
      "[ 0.483  0.044]\n",
      "Episode: 161 Total reward: -81.0463766238 Training loss: 0.9253 Explore P: 0.0508\n",
      "end episode 0.235\n",
      "Episode: 162 Total reward: 91.2211730369 Training loss: 1.0449 Explore P: 0.0500\n",
      "end episode -0.039\n",
      "Episode: 163 Total reward: 56.2579740188 Training loss: 0.9202 Explore P: 0.0492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end episode -0.159\n",
      "Episode: 164 Total reward: 74.8673928535 Training loss: 0.8146 Explore P: 0.0484\n",
      "end episode -0.211\n",
      "Episode: 165 Total reward: 50.1652207712 Training loss: 0.8981 Explore P: 0.0477\n",
      "end episode 0.276\n",
      "Episode: 166 Total reward: 77.7120435224 Training loss: 0.8962 Explore P: 0.0469\n",
      "end episode -0.34\n",
      "Episode: 167 Total reward: 36.2181201199 Training loss: 0.8748 Explore P: 0.0462\n",
      "end episode -0.344\n",
      "Episode: 168 Total reward: 12.8405844044 Training loss: 0.8353 Explore P: 0.0455\n",
      "end episode 0.508\n",
      "Done  186\n",
      "[ 0.472  0.036]\n",
      "Episode: 169 Total reward: -68.1391298715 Training loss: 0.8292 Explore P: 0.0448\n",
      "end episode -0.183\n",
      "Episode: 170 Total reward: 50.0724675235 Training loss: 0.7997 Explore P: 0.0441\n",
      "end episode -0.282\n",
      "Episode: 171 Total reward: 41.0260908997 Training loss: 0.8394 Explore P: 0.0435\n",
      "end episode 0.522\n",
      "Done  194\n",
      "[ 0.475  0.047]\n",
      "Episode: 172 Total reward: -91.3311590917 Training loss: 0.6189 Explore P: 0.0428\n",
      "end episode -0.313\n",
      "Episode: 173 Total reward: 40.5492792116 Training loss: 0.7249 Explore P: 0.0422\n",
      "end episode 0.537\n",
      "Done  176\n",
      "[ 0.487  0.049]\n",
      "Episode: 174 Total reward: -36.9601461011 Training loss: 0.7353 Explore P: 0.0416\n",
      "end episode 0.079\n",
      "Episode: 175 Total reward: 54.1123214224 Training loss: 0.7570 Explore P: 0.0410\n",
      "end episode 0.074\n",
      "Episode: 176 Total reward: 61.6753636332 Training loss: 0.7121 Explore P: 0.0404\n",
      "end episode 0.518\n",
      "Done  139\n",
      "[ 0.473  0.044]\n",
      "Episode: 177 Total reward: 6.42391233934 Training loss: 0.5915 Explore P: 0.0400\n",
      "end episode 0.532\n",
      "Done  194\n",
      "[ 0.484  0.047]\n",
      "Episode: 178 Total reward: -80.2847824679 Training loss: 0.7723 Explore P: 0.0394\n",
      "end episode 0.509\n",
      "Done  112\n",
      "[ 0.494  0.016]\n",
      "Episode: 179 Total reward: 45.9934772751 Training loss: 0.6546 Explore P: 0.0390\n",
      "end episode 0.546\n",
      "Done  147\n",
      "[ 0.498  0.048]\n",
      "Episode: 180 Total reward: 16.5565194859 Training loss: 0.5497 Explore P: 0.0386\n",
      "end episode 0.546\n",
      "Done  138\n",
      "[ 0.498  0.048]\n",
      "Episode: 181 Total reward: 31.1724610454 Training loss: 0.6014 Explore P: 0.0382\n",
      "end episode 0.537\n",
      "Done  153\n",
      "[ 0.487  0.049]\n",
      "Episode: 182 Total reward: 6.50362013712 Training loss: 0.4519 Explore P: 0.0378\n",
      "end episode 0.537\n",
      "Done  153\n",
      "[ 0.487  0.049]\n",
      "Episode: 183 Total reward: 15.788402605 Training loss: 0.6344 Explore P: 0.0374\n",
      "end episode -0.518\n",
      "Episode: 184 Total reward: 4.77535715513 Training loss: 0.5511 Explore P: 0.0368\n",
      "end episode 0.537\n",
      "Done  165\n",
      "[ 0.487  0.049]\n",
      "Episode: 185 Total reward: -32.211597395 Training loss: 0.4389 Explore P: 0.0364\n",
      "end episode -0.521\n",
      "Episode: 186 Total reward: 8.86811040278 Training loss: 0.5583 Explore P: 0.0359\n",
      "end episode 0.537\n",
      "Done  154\n",
      "[ 0.487  0.049]\n",
      "Episode: 187 Total reward: 11.5036201371 Training loss: 0.7931 Explore P: 0.0355\n",
      "end episode 0.521\n",
      "Done  150\n",
      "[ 0.471  0.049]\n",
      "Episode: 188 Total reward: 14.788402605 Training loss: 28.1170 Explore P: 0.0351\n",
      "end episode -0.456\n",
      "Episode: 189 Total reward: 5.58332793491 Training loss: 0.5866 Explore P: 0.0346\n",
      "end episode -0.454\n",
      "Episode: 190 Total reward: 7.34492209087 Training loss: 0.5586 Explore P: 0.0341\n",
      "end episode 0.523\n",
      "Done  152\n",
      "[ 0.484  0.041]\n",
      "Episode: 191 Total reward: -18.973191551 Training loss: 0.6518 Explore P: 0.0337\n",
      "end episode 0.279\n",
      "Episode: 192 Total reward: 39.1142133189 Training loss: 0.4893 Explore P: 0.0333\n",
      "end episode 0.525\n",
      "Done  128\n",
      "[ 0.481  0.045]\n",
      "Episode: 193 Total reward: 31.9405779263 Training loss: 0.6690 Explore P: 0.0330\n",
      "end episode -0.435\n",
      "Episode: 194 Total reward: 8.3514448158 Training loss: 0.5697 Explore P: 0.0325\n",
      "end episode -0.354\n",
      "Episode: 195 Total reward: 8.92100975153 Training loss: 88.8204 Explore P: 0.0321\n",
      "end episode 0.508\n",
      "Done  134\n",
      "[ 0.484  0.026]\n",
      "Episode: 196 Total reward: 1.55651948588 Training loss: 0.6978 Explore P: 0.0318\n",
      "end episode -0.09\n",
      "Episode: 197 Total reward: 75.4905746873 Training loss: 0.6253 Explore P: 0.0314\n",
      "end episode 0.506\n",
      "Done  151\n",
      "[ 0.47   0.037]\n",
      "Episode: 198 Total reward: -33.4963798629 Training loss: 0.7829 Explore P: 0.0310\n",
      "end episode 0.21\n",
      "Episode: 199 Total reward: 54.1677433431 Training loss: 67.1803 Explore P: 0.0306\n",
      "end episode -0.422\n",
      "Episode: 200 Total reward: 2.73550325624 Training loss: 95.5200 Explore P: 0.0302\n",
      "end episode 0.514\n",
      "Done  125\n",
      "[ 0.488  0.028]\n",
      "Episode: 201 Total reward: 19.7485487061 Training loss: 0.7340 Explore P: 0.0300\n",
      "end episode 0.52\n",
      "Done  146\n",
      "[ 0.5    0.022]\n",
      "Episode: 202 Total reward: -17.8739155784 Training loss: 0.6152 Explore P: 0.0297\n",
      "end episode 0.504\n",
      "Done  148\n",
      "[ 0.478  0.028]\n",
      "Episode: 203 Total reward: -8.16522077118 Training loss: 0.5149 Explore P: 0.0294\n",
      "end episode 0.015\n",
      "Episode: 204 Total reward: 72.7753571551 Training loss: 0.5416 Explore P: 0.0290\n",
      "end episode 0.509\n",
      "Done  150\n",
      "[ 0.484  0.027]\n",
      "Episode: 205 Total reward: -23.211597395 Training loss: 0.6279 Explore P: 0.0287\n",
      "end episode 0.53\n",
      "Done  148\n",
      "[ 0.494  0.037]\n",
      "Episode: 206 Total reward: 1.02680844904 Training loss: 0.5506 Explore P: 0.0284\n",
      "end episode -0.091\n",
      "Episode: 207 Total reward: 88.5369513111 Training loss: 0.8328 Explore P: 0.0281\n",
      "end episode 0.503\n",
      "Done  137\n",
      "[ 0.467  0.037]\n",
      "Episode: 208 Total reward: 14.1724610454 Training loss: 0.5726 Explore P: 0.0278\n",
      "end episode -0.005\n",
      "Episode: 209 Total reward: 84.6826039075 Training loss: 15.7891 Explore P: 0.0275\n",
      "end episode 0.519\n",
      "Done  150\n",
      "[ 0.488  0.032]\n",
      "Episode: 210 Total reward: -23.8739155784 Training loss: 0.8621 Explore P: 0.0272\n",
      "end episode 0.533\n",
      "Done  143\n",
      "[ 0.495  0.038]\n",
      "Episode: 211 Total reward: 6.17246104543 Training loss: 11.9926 Explore P: 0.0270\n",
      "end episode 0.517\n",
      "Done  137\n",
      "[ 0.475  0.043]\n",
      "Episode: 212 Total reward: 20.6492727335 Training loss: 0.5108 Explore P: 0.0267\n",
      "end episode 0.235\n",
      "Episode: 213 Total reward: 72.0434869402 Training loss: 0.6398 Explore P: 0.0264\n",
      "end episode 0.516\n",
      "Done  114\n",
      "[ 0.476  0.041]\n",
      "Episode: 214 Total reward: 56.033331174 Training loss: 70.1294 Explore P: 0.0262\n",
      "end episode -0.033\n",
      "Episode: 215 Total reward: 58.2057922194 Training loss: 0.7383 Explore P: 0.0259\n",
      "end episode 0.327\n",
      "Episode: 216 Total reward: 88.127916352 Training loss: 0.7660 Explore P: 0.0256\n",
      "end episode 0.006\n",
      "Episode: 217 Total reward: 53.4905746873 Training loss: 61.7079 Explore P: 0.0253\n",
      "end episode -0.594\n",
      "Episode: 218 Total reward: 6.060139623 Training loss: 0.7297 Explore P: 0.0250\n",
      "end episode -0.498\n",
      "Episode: 219 Total reward: 2.29854546705 Training loss: 0.8820 Explore P: 0.0247\n",
      "end episode -0.545\n",
      "Episode: 220 Total reward: 21.0137629992 Training loss: 0.8403 Explore P: 0.0244\n",
      "end episode 0.524\n",
      "Done  108\n",
      "[ 0.488  0.035]\n",
      "Episode: 221 Total reward: 74.9869545501 Training loss: 0.7055 Explore P: 0.0242\n",
      "end episode 0.537\n",
      "Done  154\n",
      "[ 0.487  0.049]\n",
      "Episode: 222 Total reward: 2.54999676095 Training loss: 0.6563 Explore P: 0.0240\n",
      "end episode -0.089\n",
      "Episode: 223 Total reward: 49.543474036 Training loss: 0.9161 Explore P: 0.0237\n",
      "end episode 0.519\n",
      "Done  158\n",
      "[ 0.471  0.048]\n",
      "Episode: 224 Total reward: 10.6661188814 Training loss: 0.6370 Explore P: 0.0235\n",
      "end episode 0.512\n",
      "Done  151\n",
      "[ 0.468  0.044]\n",
      "Episode: 225 Total reward: 4.26521429308 Training loss: 0.3896 Explore P: 0.0233\n",
      "end episode -0.508\n",
      "Episode: 226 Total reward: -2.27101946869 Training loss: 0.7054 Explore P: 0.0231\n",
      "end episode 0.019\n",
      "Episode: 227 Total reward: 91.1594155956 Training loss: 8.2795 Explore P: 0.0228\n",
      "end episode 0.504\n",
      "Done  108\n",
      "[ 0.493  0.013]\n",
      "Episode: 228 Total reward: 46.5630422108 Training loss: 0.7164 Explore P: 0.0227\n",
      "end episode 0.537\n",
      "Done  151\n",
      "[ 0.487  0.049]\n",
      "Episode: 229 Total reward: 14.8608677432 Training loss: 0.6715 Explore P: 0.0225\n",
      "end episode 0.537\n",
      "Done  137\n",
      "[ 0.487  0.049]\n",
      "Episode: 230 Total reward: 22.4108668895 Training loss: 0.7035 Explore P: 0.0223\n",
      "end episode 0.537\n",
      "Done  152\n",
      "[ 0.487  0.049]\n",
      "Episode: 231 Total reward: 5.89935153277 Training loss: 0.7204 Explore P: 0.0221\n",
      "end episode 0.53\n",
      "Done  135\n",
      "[ 0.494  0.037]\n",
      "Episode: 232 Total reward: 12.4108668895 Training loss: 0.6662 Explore P: 0.0219\n",
      "end episode 0.506\n",
      "Done  146\n",
      "[ 0.456  0.049]\n",
      "Episode: 233 Total reward: 13.8347792288 Training loss: 0.8879 Explore P: 0.0218\n",
      "end episode -0.541\n",
      "Episode: 234 Total reward: 9.58332793491 Training loss: 1.0296 Explore P: 0.0215\n",
      "end episode -0.144\n",
      "Episode: 235 Total reward: 72.298545467 Training loss: 0.9782 Explore P: 0.0213\n",
      "end episode 0.544\n",
      "Done  134\n",
      "[ 0.496  0.047]\n",
      "Episode: 236 Total reward: 32.3644902657 Training loss: 0.7231 Explore P: 0.0212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end episode 0.518\n",
      "Done  140\n",
      "[ 0.47   0.048]\n",
      "Episode: 237 Total reward: 23.026808449 Training loss: 0.9346 Explore P: 0.0210\n",
      "end episode 0.517\n",
      "Done  148\n",
      "[ 0.468  0.048]\n",
      "Episode: 238 Total reward: 16.788402605 Training loss: 0.7867 Explore P: 0.0208\n",
      "end episode -0.545\n",
      "Episode: 239 Total reward: 12.8681104028 Training loss: 0.4998 Explore P: 0.0206\n",
      "end episode 0.507\n",
      "Done  151\n",
      "[ 0.46   0.047]\n",
      "Episode: 240 Total reward: -18.6884090831 Training loss: 0.7435 Explore P: 0.0205\n",
      "end episode -0.5\n",
      "Episode: 241 Total reward: 23.2521688432 Training loss: 0.6241 Explore P: 0.0203\n",
      "end episode 0.537\n",
      "Done  144\n",
      "[ 0.487  0.049]\n",
      "Episode: 242 Total reward: 8.3115909169 Training loss: 0.5702 Explore P: 0.0201\n",
      "end episode 0.517\n",
      "Done  110\n",
      "[ 0.499  0.019]\n",
      "Episode: 243 Total reward: 43.2253603942 Training loss: 0.6501 Explore P: 0.0200\n",
      "end episode 0.537\n",
      "Done  141\n",
      "[ 0.487  0.049]\n",
      "Episode: 244 Total reward: 21.2188376693 Training loss: 0.8303 Explore P: 0.0199\n",
      "end episode 0.537\n",
      "Done  152\n",
      "[ 0.487  0.049]\n",
      "Episode: 245 Total reward: 6.22025090984 Training loss: 0.4680 Explore P: 0.0197\n",
      "end episode 0.537\n",
      "Done  148\n",
      "[ 0.487  0.049]\n",
      "Episode: 246 Total reward: 4.3115909169 Training loss: 0.6635 Explore P: 0.0196\n",
      "end episode -0.568\n",
      "Episode: 247 Total reward: 9.34492209087 Training loss: 0.8888 Explore P: 0.0194\n",
      "end episode 0.517\n",
      "Done  145\n",
      "[ 0.468  0.048]\n",
      "Episode: 248 Total reward: -1.49637986288 Training loss: 0.8553 Explore P: 0.0192\n",
      "end episode 0.537\n",
      "Done  142\n",
      "[ 0.487  0.049]\n",
      "Episode: 249 Total reward: 23.5036201371 Training loss: 1.1017 Explore P: 0.0191\n",
      "end episode 0.537\n",
      "Done  140\n",
      "[ 0.487  0.049]\n",
      "Episode: 250 Total reward: 24.2188376693 Training loss: 67.1156 Explore P: 0.0190\n",
      "end episode -0.097\n",
      "Episode: 251 Total reward: 75.4905746873 Training loss: 0.5833 Explore P: 0.0188\n",
      "end episode 0.537\n",
      "Done  147\n",
      "[ 0.487  0.049]\n",
      "Episode: 252 Total reward: 7.54999676095 Training loss: 0.6046 Explore P: 0.0187\n",
      "end episode -0.579\n",
      "Episode: 253 Total reward: 4.060139623 Training loss: 0.4615 Explore P: 0.0185\n",
      "end episode 0.536\n",
      "Done  154\n",
      "[ 0.487  0.049]\n",
      "Episode: 254 Total reward: 2.56324566093 Training loss: 0.7334 Explore P: 0.0184\n",
      "end episode 0.536\n",
      "Done  145\n",
      "[ 0.487  0.049]\n",
      "Episode: 255 Total reward: 1.78840260499 Training loss: 0.7384 Explore P: 0.0183\n",
      "end episode 0.503\n",
      "Done  149\n",
      "[ 0.455  0.048]\n",
      "Episode: 256 Total reward: -2.6884090831 Training loss: 0.6223 Explore P: 0.0181\n",
      "end episode -0.564\n",
      "Episode: 257 Total reward: 11.1065162468 Training loss: 0.5471 Explore P: 0.0180\n",
      "end episode 0.537\n",
      "Done  144\n",
      "[ 0.487  0.049]\n",
      "Episode: 258 Total reward: 18.5499967609 Training loss: 0.6302 Explore P: 0.0179\n",
      "end episode 0.501\n",
      "Done  143\n",
      "[ 0.452  0.048]\n",
      "Episode: 259 Total reward: 12.5036201371 Training loss: 0.6730 Explore P: 0.0177\n",
      "end episode 0.537\n",
      "Done  144\n",
      "[ 0.487  0.049]\n",
      "Episode: 260 Total reward: 12.0731850729 Training loss: 0.5921 Explore P: 0.0176\n",
      "end episode 0.513\n",
      "Done  143\n",
      "[ 0.479  0.035]\n",
      "Episode: 261 Total reward: 1.54999676095 Training loss: 0.6799 Explore P: 0.0175\n",
      "end episode 0.524\n",
      "Done  154\n",
      "[ 0.493  0.031]\n",
      "Episode: 262 Total reward: -34.4036266152 Training loss: 2.6528 Explore P: 0.0174\n",
      "end episode -0.554\n",
      "Episode: 263 Total reward: 12.6297045587 Training loss: 0.4019 Explore P: 0.0173\n",
      "end episode 0.517\n",
      "Done  148\n",
      "[ 0.475  0.043]\n",
      "Episode: 264 Total reward: -3.6884090831 Training loss: 0.6141 Explore P: 0.0172\n",
      "end episode 0.524\n",
      "Done  144\n",
      "[ 0.493  0.031]\n",
      "Episode: 265 Total reward: -2.73478570692 Training loss: 64.3684 Explore P: 0.0171\n",
      "end episode 0.513\n",
      "Done  135\n",
      "[ 0.479  0.035]\n",
      "Episode: 266 Total reward: 13.9340552014 Training loss: 0.6293 Explore P: 0.0170\n",
      "end episode 0.513\n",
      "Done  147\n",
      "[ 0.496  0.018]\n",
      "Episode: 267 Total reward: -24.5427564867 Training loss: 0.6432 Explore P: 0.0169\n",
      "end episode 0.524\n",
      "Done  143\n",
      "[ 0.493  0.031]\n",
      "Episode: 268 Total reward: -1.97319155096 Training loss: 0.8982 Explore P: 0.0168\n",
      "end episode 0.524\n",
      "Done  146\n",
      "[ 0.493  0.031]\n",
      "Episode: 269 Total reward: -7.21159739501 Training loss: 80.4192 Explore P: 0.0167\n",
      "end episode 0.525\n",
      "Done  145\n",
      "[ 0.499  0.027]\n",
      "Episode: 270 Total reward: -13.973191551 Training loss: 0.7790 Explore P: 0.0166\n",
      "end episode 0.524\n",
      "Done  152\n",
      "[ 0.493  0.031]\n",
      "Episode: 271 Total reward: -34.7347857069 Training loss: 0.9499 Explore P: 0.0165\n",
      "end episode 0.51\n",
      "Done  139\n",
      "[ 0.485  0.026]\n",
      "Episode: 272 Total reward: -2.30435064265 Training loss: 0.7563 Explore P: 0.0164\n",
      "end episode 0.519\n",
      "Done  142\n",
      "[ 0.491  0.029]\n",
      "Episode: 273 Total reward: -6.01956817479 Training loss: 0.5067 Explore P: 0.0163\n",
      "end episode 0.5\n",
      "Done  150\n",
      "[ 0.476  0.026]\n",
      "Episode: 274 Total reward: -25.4500032391 Training loss: 0.3429 Explore P: 0.0162\n",
      "end episode 0.503\n",
      "Done  145\n",
      "[ 0.487  0.017]\n",
      "Episode: 275 Total reward: -20.0659447986 Training loss: 0.8772 Explore P: 0.0161\n",
      "end episode 0.53\n",
      "Done  142\n",
      "[ 0.494  0.037]\n",
      "Episode: 276 Total reward: 3.98043182521 Training loss: 2.6607 Explore P: 0.0160\n",
      "end episode 0.522\n",
      "Done  144\n",
      "[ 0.491  0.032]\n",
      "Episode: 277 Total reward: -6.01956817479 Training loss: 0.4653 Explore P: 0.0159\n",
      "end episode 0.513\n",
      "Done  143\n",
      "[ 0.479  0.035]\n",
      "Episode: 278 Total reward: 2.02680844904 Training loss: 0.6067 Explore P: 0.0158\n",
      "end episode 0.514\n",
      "Done  143\n",
      "[ 0.484  0.031]\n",
      "Episode: 279 Total reward: -0.688409083097 Training loss: 0.7113 Explore P: 0.0158\n",
      "end episode -0.546\n",
      "Episode: 280 Total reward: 7.10651624682 Training loss: 0.5137 Explore P: 0.0156\n",
      "end episode -0.133\n",
      "Episode: 281 Total reward: 78.3514448158 Training loss: 0.6318 Explore P: 0.0155\n",
      "end episode 0.53\n",
      "Done  146\n",
      "[ 0.494  0.037]\n",
      "Episode: 282 Total reward: -3.92681492714 Training loss: 1.6093 Explore P: 0.0155\n",
      "end episode 0.506\n",
      "Done  141\n",
      "[ 0.467  0.041]\n",
      "Episode: 283 Total reward: 11.5499967609 Training loss: 0.6463 Explore P: 0.0154\n",
      "end episode 0.504\n",
      "Done  145\n",
      "[ 0.475  0.031]\n",
      "Episode: 284 Total reward: -6.6884090831 Training loss: 0.5764 Explore P: 0.0153\n",
      "end episode 0.525\n",
      "Done  147\n",
      "[ 0.499  0.027]\n",
      "Episode: 285 Total reward: -19.7347857069 Training loss: 0.4923 Explore P: 0.0152\n",
      "end episode 0.506\n",
      "Done  153\n",
      "[ 0.469  0.038]\n",
      "Episode: 286 Total reward: -31.7347857069 Training loss: 22.2423 Explore P: 0.0151\n",
      "end episode 0.363\n",
      "Episode: 287 Total reward: 58.4815410726 Training loss: 0.5189 Explore P: 0.0150\n",
      "end episode -0.055\n",
      "Episode: 288 Total reward: 62.6826039075 Training loss: 0.7470 Explore P: 0.0149\n",
      "end episode -0.579\n",
      "Episode: 289 Total reward: 5.58332793491 Training loss: 2.8475 Explore P: 0.0148\n",
      "end episode 0.506\n",
      "Done  151\n",
      "[ 0.46   0.046]\n",
      "Episode: 290 Total reward: -29.1652207712 Training loss: 0.6414 Explore P: 0.0148\n",
      "end episode 0.539\n",
      "Done  145\n",
      "[ 0.492  0.047]\n",
      "Episode: 291 Total reward: 2.02680844904 Training loss: 0.4313 Explore P: 0.0147\n",
      "end episode 0.534\n",
      "Done  139\n",
      "[ 0.486  0.048]\n",
      "Episode: 292 Total reward: 17.2188376693 Training loss: 0.1945 Explore P: 0.0146\n",
      "end episode 0.506\n",
      "Done  140\n",
      "[ 0.467  0.041]\n",
      "Episode: 293 Total reward: 15.5036201371 Training loss: 8.5159 Explore P: 0.0146\n",
      "end episode 0.513\n",
      "Done  144\n",
      "[ 0.479  0.035]\n",
      "Episode: 294 Total reward: 4.07318507286 Training loss: 0.9683 Explore P: 0.0145\n",
      "end episode 0.535\n",
      "Done  143\n",
      "[ 0.487  0.049]\n",
      "Episode: 295 Total reward: 14.7420259812 Training loss: 1.8914 Explore P: 0.0144\n",
      "end episode 0.517\n",
      "Done  140\n",
      "[ 0.475  0.043]\n",
      "Episode: 296 Total reward: 13.7420259812 Training loss: 0.5953 Explore P: 0.0144\n",
      "end episode 0.506\n",
      "Done  139\n",
      "[ 0.467  0.041]\n",
      "Episode: 297 Total reward: 11.6956493573 Training loss: 0.4584 Explore P: 0.0143\n",
      "end episode 0.53\n",
      "Done  144\n",
      "[ 0.494  0.037]\n",
      "Episode: 298 Total reward: 1.50362013712 Training loss: 0.4587 Explore P: 0.0143\n",
      "end episode 0.506\n",
      "Done  142\n",
      "[ 0.467  0.041]\n",
      "Episode: 299 Total reward: 7.98043182521 Training loss: 0.4169 Explore P: 0.0142\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    max_state = -2.0\n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            #env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape(1, *state.shape)}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            #print_func(state,action)\n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state[0] = round(next_state[0], 3)\n",
    "            next_state[1] = round(next_state[1], 3)\n",
    "            reward = reward_function(next_state, action)\n",
    "            #print(reward)\n",
    "            total_reward += reward\n",
    "            if next_state[0] > max_state:\n",
    "                max_state = next_state[0]\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                print('end episode', max_state)\n",
    "                \n",
    "                if t < 198:\n",
    "                    print('Done ',t)\n",
    "                    total_reward += 200 - (t*2)\n",
    "                    print(state)\n",
    "                t = max_steps\n",
    "                max_state = -2.0\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "                state[0] = round(state[0], 3)\n",
    "                state[1] = round(state[1], 3)\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0,0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "            \n",
    "\n",
    "        saver.save(sess, \"checkpoints/mountain.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047425873177566788"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "expit(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0] =round(state[0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.694     , -0.00251189])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58005696,  0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/mountain.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-13 13:22:04,540] Restoring parameters from checkpoints/mountain.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.455\n",
      "-0.458\n",
      "-0.462\n",
      "-0.468\n",
      "-0.476\n",
      "-0.485\n",
      "-0.495\n",
      "-0.506\n",
      "-0.518\n",
      "-0.532\n",
      "-0.546\n",
      "-0.561\n",
      "-0.577\n",
      "-0.594\n",
      "-0.611\n",
      "-0.628\n",
      "-0.646\n",
      "-0.663\n",
      "-0.681\n",
      "-0.699\n",
      "-0.716\n",
      "-0.733\n",
      "-0.749\n",
      "-0.765\n",
      "-0.78\n",
      "-0.795\n",
      "-0.808\n",
      "-0.821\n",
      "-0.833\n",
      "-0.844\n",
      "-0.854\n",
      "-0.862\n",
      "-0.87\n",
      "-0.876\n",
      "-0.882\n",
      "-0.886\n",
      "-0.886\n",
      "-0.884\n",
      "-0.878\n",
      "-0.87\n",
      "-0.858\n",
      "-0.843\n",
      "-0.825\n",
      "-0.804\n",
      "-0.78\n",
      "-0.753\n",
      "-0.724\n",
      "-0.692\n",
      "-0.658\n",
      "-0.622\n",
      "-0.585\n",
      "-0.546\n",
      "-0.506\n",
      "-0.465\n",
      "-0.423\n",
      "-0.381\n",
      "-0.339\n",
      "-0.298\n",
      "-0.257\n",
      "-0.217\n",
      "-0.177\n",
      "-0.139\n",
      "-0.105\n",
      "-0.073\n",
      "-0.045\n",
      "-0.021\n",
      "0.0\n",
      "0.017\n",
      "0.031\n",
      "0.042\n",
      "0.049\n",
      "0.052\n",
      "0.052\n",
      "0.049\n",
      "0.042\n",
      "0.032\n",
      "0.018\n",
      "0.0\n",
      "-0.02\n",
      "-0.045\n",
      "-0.073\n",
      "-0.104\n",
      "-0.139\n",
      "-0.176\n",
      "-0.218\n",
      "-0.262\n",
      "-0.309\n",
      "-0.358\n",
      "-0.409\n",
      "-0.463\n",
      "-0.518\n",
      "-0.574\n",
      "-0.63\n",
      "-0.687\n",
      "-0.743\n",
      "-0.799\n",
      "-0.855\n",
      "-0.909\n",
      "-0.961\n",
      "-1.013\n",
      "-1.063\n",
      "-1.111\n",
      "-1.158\n",
      "-1.2\n",
      "-1.197\n",
      "-1.19\n",
      "-1.18\n",
      "-1.167\n",
      "-1.151\n",
      "-1.131\n",
      "-1.108\n",
      "-1.081\n",
      "-1.051\n",
      "-1.018\n",
      "-0.98\n",
      "-0.94\n",
      "-0.896\n",
      "-0.849\n",
      "-0.798\n",
      "-0.745\n",
      "-0.69\n",
      "-0.632\n",
      "-0.572\n",
      "-0.511\n",
      "-0.449\n",
      "-0.387\n",
      "-0.324\n",
      "-0.262\n",
      "-0.201\n",
      "-0.141\n",
      "-0.082\n",
      "-0.025\n",
      "0.031\n",
      "0.086\n",
      "0.139\n",
      "0.19\n",
      "0.241\n",
      "0.289\n",
      "0.334\n",
      "0.377\n",
      "0.417\n",
      "0.456\n",
      "0.494\n",
      "Very good\n",
      "Episode: 0 Total reward: 0 Training loss: 4.8828 Explore P: 0.0142\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9a00029bc83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                       \u001b[0;34m'Training loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                       'Explore P: {:.4f}'.format(explore_p))\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = tf.train.import_meta_graph('checkpoints/mountain.ckpt.meta')\n",
    "    model = model.restore(sess,tf.train.latest_checkpoint('checkpoints/'))\n",
    "    step=0\n",
    "    \n",
    "    for ep in range(1):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            t +=1\n",
    "            env.render()\n",
    "            feed = {mainQN.inputs_: state.reshape(1, *state.shape)}\n",
    "            Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "                       \n",
    "            #print_func(state,action)\n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state[0] = round(next_state[0], 3)\n",
    "            next_state[1] = round(next_state[1], 3)\n",
    "            print(state[0])\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"Very good\")\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                r\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
